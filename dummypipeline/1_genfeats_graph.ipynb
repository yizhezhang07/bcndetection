{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Graph DB and Topological features\n",
    "- This notebook generates the graph-based features that is updated on a daily bases\n",
    "- The notebook also shows how we add notes to the graph database.\n",
    "- At the end of this notebook, we concat all computed features (in the two notebooks). \n",
    "- **Note** Run notebook `0_genfeats_nongraph.ipynb` before this one, as the graph data depends on some files generated from that notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph DB and Topological features\n",
    "---\n",
    "This notebook shows how we add nodes into neo4j database and compute graphical features:\n",
    "- We provide a dummy dataset:\n",
    "    - `dummy100.csv`: we sampled 100 rows of non-user-sensitive periodic FQDNs from 2021-03-30. We carefully examined every host name to ensure there's no privacy leakage. We restricted FQDNs to domains that have periodic activity because non-periodic domains are filtered out before feature generation in daily pipeline.\n",
    "    - The file contains the host name, server IP and port extracted from Zeek logs. Client IPs are faked to avoid privacy leakage.\n",
    "    - `periodic100.parquet`: the corresponding periodicity detection results of the 100 servers.\n",
    "    - `hist100.parquet`: the history file we just computed in `0_genfeats_nongraph.ipynb`.\n",
    "    - `cisco_top1m.csv`: the cisco top 1 million data we pulled on 2021-03-30.\n",
    "    - `malicious_hist.csv`: this sampled file keeps track of historical malicious servers.\n",
    "- The below code reads the above dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "host100 = pd.read_csv(\"dummydata/dummy100.csv\")\n",
    "per100 = pd.read_parquet(\"dummydata/periodic100.parquet\")\n",
    "cisco_1m = pd.read_csv(\"dummydata/cisco_top1m.csv\", names=[\"rank\", \"host\"])\n",
    "hist100 = pd.read_parquet(\"dummydata/hist100.parquet\") \n",
    "malhist = pd.read_csv(\"dummydata/malicious_hist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Important] Neo4j GraphDB\n",
    "----\n",
    "- If you have configured the graph database as described in the README.md, skip the following installation guidance.\n",
    "\n",
    "#### Neo4j GraphDB Installation and Configuration\n",
    "- Before we start to compute topological features, we need to install and configure graph database. The following code was deployed on neo4j graph database community version 4.4.0\n",
    "- Offical installation guidance can be found at: https://debian.neo4j.com/?_ga=2.50993706.2051114848.1695324818-590444767.1695324818\n",
    "\n",
    "- To install neo4j 4.4.0:\n",
    "    - `wget -O - https://debian.neo4j.com/neotechnology.gpg.key | sudo apt-key add -`\n",
    "    - `echo 'deb https://debian.neo4j.com stable 4.4' | sudo tee /etc/apt/sources.list.d/neo4j.list`\n",
    "    - `sudo apt-get update`\n",
    "    - `sudo apt-get install neo4j=1:4.4.0`\n",
    "- After installation complete, check the installation status in the command line: \n",
    "    - `neo4j status`\n",
    "    - if you see something like `neo4j is not running`, then we have a successful installation.\n",
    "- Next we need to start the graph db:\n",
    "    - `sudo neo4j start`\n",
    "- After we start the neo4j database, you need to change the default password of neo4j. Type command: \n",
    "    - `sudo cypher-shell`\n",
    "    - If you see `connection refused`, wait another 20 seconds and retry. There is a short delay after starting the graph db.\n",
    "    - The default username is:`neo4j`, and the default password is:`neo4j`. Change the password to your own password.\n",
    "    - If you are using our test server, the default password is set to `acsac`.\n",
    "- Leave the `cypher-shell` using `Ctrl + D`\n",
    "- **Important.** We need to config the database configuration before we start the database. There are two approaches:\n",
    "    - **First approach**: \n",
    "        - you can copy our provided configuration to rewrite the configurate.\n",
    "        - `sudo mv /etc/neo4j/neo4j.conf /etc/neo4j/neo4j_bk.conf`\n",
    "        - `sudo cp /home/ubuntu/bcndetection/dummypipeline/neo4j.conf /etc/neo4j/neo4j.conf`\n",
    "    - **Second approach**: \n",
    "        - you can directly edit the file by navigating to the default config file at `/etc/neo4j/neo4j.conf`. \n",
    "        - You need **sudo** to edit this file, e.g. `sudo vim /etc/neo4j/neo4j.conf`\n",
    "        - **Change:** `dbms.directories.import=$path_to_repo$/dummypipeline/dummydata/graphtmp`\n",
    "        - For example: `dbms.directories.import=/home/ubuntu/bcndetection/dummypipeline/dummydata/graphtmp`\n",
    "        - Next, we need to **add** the following `dbms.security.procedures.unrestricted=apoc.*` in the neo4j.conf\n",
    "        - **Save the configuration file.**\n",
    "- **Important.** Install Neo4j APOC Core functions:\n",
    "    - `sudo mv /var/lib/neo4j/labs/apoc-*core.jar /var/lib/neo4j/plugins/`\n",
    "- **Important.** Configure Neo4j APOC function:\n",
    "    - copy the provided configuration file to the same neo4j configuration folder:\n",
    "    - `sudo cp /home/ubuntu/bcndetection/dummypipeline/apoc.conf /etc/neo4j/apoc.conf`\n",
    "- **Important.** We need to restart the database after the configuration with command: `sudo neo4j restart`\n",
    "- By default, you can visualize the graphdb at: `http://localhost:7474/browser/`\n",
    "\n",
    "- Run the below code to add nodes into the database and compute related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tldextract\n",
    "from neo4j import GraphDatabase\n",
    "from src.addnode import AddNode\n",
    "from src.temporal_feats import merge_cisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_graph_inputdata(logdf, ciscodf, logday, fqdn_col=\"host\"):\n",
    "    logdf = merge_cisco(logdf, ciscodf)\n",
    "    logdf[\"logday\"] = logday\n",
    "    logdf[\"domain\"] = logdf[fqdn_col].apply(lambda x: parse_reg_dom(x))\n",
    "    logdf[\"isIP\"] = logdf[fqdn_col].apply(lambda x: isIP(x))\n",
    "    return logdf\n",
    "\n",
    "    \n",
    "def parse_reg_dom(text):\n",
    "    try:\n",
    "        ext = tldextract.extract(text)\n",
    "        if ext.registered_domain == '':\n",
    "            return text\n",
    "        return ext.registered_domain\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "\n",
    "def isIP(dom_text):\n",
    "    \"\"\"check if string is an ip address\"\"\"\n",
    "    try:\n",
    "        _ = ipaddress.ip_address(dom_text)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path configuration and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save data to graph db tmp dir: dummydata/graphtmp/data_2021-03-30.csv\n",
      "relative fpath for data: file:///data_2021-03-30.csv\n",
      "relative fpath for historical malicious file: file:///malicious_hist.csv\n"
     ]
    }
   ],
   "source": [
    "logday = \"2021-03-30\"\n",
    "data_fmt = \"data_{}.csv\"\n",
    "feature_dir = \"dummydata/features/{}\".format(logday)\n",
    "\n",
    "# set tmp dir for graph db to read and write\n",
    "graphtmpdir = \"dummydata/graphtmp/\" \n",
    "# set the relative path, this value is defined by the $dbms.directories.import in the neo4j.conf\n",
    "graphimport_rel = \"file:///\" \n",
    "\n",
    "# we first prepare the data that we want to add to the graphdb\n",
    "logdf = host100.drop(columns=[\"id_orig_h\"])\n",
    "graphdf = prepare_graph_inputdata(logdf, cisco_1m, logday)\n",
    "\n",
    "# next we save it to graphtmp folder\n",
    "# Neo4j can directly read files in the import folder specified by $dbms.directories.import\n",
    "data_fpath = os.path.join(graphtmpdir, data_fmt.format(logday))\n",
    "print(\"save data to graph db tmp dir:\", data_fpath)\n",
    "graphdf.to_csv(data_fpath, index=False)\n",
    "\n",
    "# next we set up fpath for data, historical malicious files, and features\n",
    "graph_relfpath = os.path.join(graphimport_rel, data_fmt.format(logday))\n",
    "print(\"relative fpath for data:\", graph_relfpath)\n",
    "histmal_relfpath = os.path.join(graphimport_rel, \"malicious_hist.csv\")\n",
    "print(\"relative fpath for historical malicious file:\", histmal_relfpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Next we perform a series of computation in graph db\n",
    "- **Important** make sure the graphdb is started before run the following cells.\n",
    "    - to check the neo4j status: `sudo neo4j status`\n",
    "    - to start the neo4j db: `sudo neo4j start` \n",
    "    - wait for ~30 seconds before running the cell, if the graph db is just started. \n",
    "- we add FQDN, Domain, IP and corresponding relationships.\n",
    "- we update the graph db using historical malicious files.\n",
    "- we compute graph-related features and save them in files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the password in the following cell to connect to graphdb\n",
    "- if you are using our provided testing server, the username is neo4j and the password is acsac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### change the password (third argument) to your plaintext password\n",
    "dbhandler = AddNode(\"neo4j:anon_resphost:7687\", \"neo4j\", \"acsac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Adding Nodes------\n",
      "------Finish Adding Nodes------\n",
      "<Record count(f)=100>\n",
      "----------Updating GraphDB Lables---------\n",
      "----------Finish Updating GraphDB Lables---------\n",
      "<Record count(f)=5437>\n",
      "----------Updating GraphDB Domain Features---------\n",
      "----------Finish Updating GraphDB Domain Features---------\n",
      "<Record count(d)=88>\n",
      "----------Updating GraphDB IP Features---------\n",
      "----------Finish Updating GraphDB IP Features---------\n",
      "<Record count(i)=98>\n"
     ]
    }
   ],
   "source": [
    "### add FQDN, Domain, IP and corresponding relationships\n",
    "dbhandler.add_nodes(graph_relfpath)\n",
    "\n",
    "### update the graph db using historical malicious files.\n",
    "dbhandler.update_labels(histmal_relfpath)\n",
    "\n",
    "## we compute graph-related features.\n",
    "## both function compute statistics from neighboring nodes\n",
    "dbhandler.update_domMal_feats()\n",
    "dbhandler.update_ipMal_feats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Writing GraphDB Domain Features---------\n",
      "----------Finish Writing GraphDB Domain Features---------\n",
      "<Record file='/home/ubuntu/bcndetection/dummypipeline/dummydata/features/2021-03-30/domscore_raw.csv' source='statement: cols(15)' format='csv' nodes=0 relationships=0 properties=1500 time=7 rows=100 batchSize=20000>\n",
      "----------Writing GraphDB IP Features---------\n",
      "----------Finish Writing GraphDB IP Features---------\n",
      "<Record file='/home/ubuntu/bcndetection/dummypipeline/dummydata/features/2021-03-30/ipscore_raw.csv' source='statement: cols(15)' format='csv' nodes=0 relationships=0 properties=1470 time=5 rows=98 batchSize=20000>\n"
     ]
    }
   ],
   "source": [
    "## we save the computed featues to files\n",
    "## absolute path here\n",
    "domscore_fpath = os.path.join(\"/home/ubuntu/bcndetection/dummypipeline/dummydata/features/2021-03-30\", \"domscore_raw.csv\")\n",
    "ipscore_fpath = os.path.join(\"/home/ubuntu/bcndetection/dummypipeline/dummydata/features/2021-03-30\", \"ipscore_raw.csv\")\n",
    "\n",
    "dbhandler.domscore_to_csv(domscore_fpath)\n",
    "dbhandler.ipscore_to_csv(ipscore_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Close the database\n",
    "dbhandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute graph-based features\n",
    "- given the raw score, we compute graph based features:\n",
    "    - domain-related features \n",
    "    - ip-related features\n",
    "    - distance to nearest malicious neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.graphscore_feats import gen_dom_graphscore, gen_ip_graphscore, gen_len2mal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]====== Generate Domain Graph Features for date: 2021-03-30 ======\n",
      "[Info] Raw Data shape: (100, 2)\n",
      "(100, 18)\n",
      "[Info] Graph Domain Features Save Data to: dummydata/features/2021-03-30/features_domgraph.parquet\n"
     ]
    }
   ],
   "source": [
    "domscore = gen_dom_graphscore(logday, datafpath=data_fpath, \n",
    "                              savefpath=os.path.join(feature_dir, \"features_domgraph.parquet\"),\n",
    "                              rawfpath=os.path.join(feature_dir, \"domscore_raw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]====== Generate IP Graph Features for date: 2021-03-30 ======\n",
      "[Info] Raw Data shape: (100, 2)\n",
      "[Info] IP features: (100, 19)\n",
      "[Info] Graph IP Features Save Data to: dummydata/features/2021-03-30/features_ipgraph.parquet\n"
     ]
    }
   ],
   "source": [
    "ipscore = gen_ip_graphscore(logday, datafpath=data_fpath,\n",
    "                            savefpath=os.path.join(feature_dir, \"features_ipgraph.parquet\"),\n",
    "                            rawfpath=os.path.join(feature_dir, \"ipscore_raw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]====== Generate Graph Connection Features for date: 2021-03-30 ======\n",
      "[Info] Read historical malicious data: dummydata/graphtmp/malicious_hist.csv\n",
      "[Info] Read Raw Data from:  dummydata/graphtmp/data_2021-03-30.csv\n",
      "[Info] Raw Data shape: (100, 3)\n",
      "[Info] Features Shape (100, 4)\n",
      "[Info] Graph Connection Features Save Data to: dummydata/features/2021-03-30/features_len2mal.parquet\n"
     ]
    }
   ],
   "source": [
    "len2mal = gen_len2mal_score(logday, datafpath=data_fpath,\n",
    "                            savefpath=os.path.join(feature_dir, \"features_len2mal.parquet\"),\n",
    "                            histmalfpath=\"dummydata/graphtmp/malicious_hist.csv\",\n",
    "                            domscore_fpath=os.path.join(feature_dir, \"domscore_raw.csv\"),\n",
    "                            ipscore_fpath=os.path.join(feature_dir, \"ipscore_raw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we concat all features (graph and non-graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_fmt = \"features_{}.parquet\"\n",
    "features_typs = [\"domgraph\", \"ipgraph\", \"histmal\", \"hist\",\n",
    "                 \"fqdn\", \"per\", \"len2mal\"]\n",
    "drop_col = ['domain', 'id_resp_h', 'true_periods', 'total_maleng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading features domgraph from:  dummydata/features/2021-03-30/features_domgraph.parquet\n",
      "Index(['host', 'domain', 'cntMalFQDNs', 'cntIP', 'cntFQDN', 'sumMalEng',\n",
      "       'maxMalEng', 'avgMalEng', 'minMalEng', 'maxCisco', 'minCisco',\n",
      "       'avgCisco', 'sumMal', 'maxMal', 'minMal', 'avgMal', 'malFQDN_ratio',\n",
      "       'malEng_ratio'],\n",
      "      dtype='object')\n",
      "drop domain\n",
      "reading features ipgraph from:  dummydata/features/2021-03-30/features_ipgraph.parquet\n",
      "Index(['host', 'sum_ipDom', 'sum_ipMalDom', 'avg_ipDom', 'avg_ipMalDom',\n",
      "       'max_ipDom', 'max_ipMalDom', 'min_ipDom', 'min_ipMalDom',\n",
      "       'max_ipMalDomRatio', 'max_ipDomMalEngRatio', 'min_ipMalDomRatio',\n",
      "       'min_ipDomMalEngRatio', 'avg_ipMalDomRatio', 'avg_ipDomMalEngRatio',\n",
      "       'sum_ipDomMalEng', 'max_ipDomMalEng', 'min_ipDomMalEng',\n",
      "       'avg_ipDomMalEng'],\n",
      "      dtype='object')\n",
      "reading features histmal from:  dummydata/features/2021-03-30/features_histmal.parquet\n",
      "Index(['host', 'hist_malscore_min_period', 'hist_malscore_max_period',\n",
      "       'hist_malscore_mean_period', 'hist_malscore_median_period',\n",
      "       'hist_malscore_ratio_period'],\n",
      "      dtype='object')\n",
      "reading features hist from:  dummydata/features/2021-03-30/features_hist.parquet\n",
      "Index(['host', 'freq', 'occ'], dtype='object')\n",
      "reading features fqdn from:  dummydata/features/2021-03-30/features_fqdn.parquet\n",
      "Index(['host', 'psd_ratio', 'dom_illegal', 'dom_sld_entropy', 'subdom_entropy',\n",
      "       'dom_entropy', 'fqdn_entropy', 'dom_tldcnt', 'dom_sldcnt', 'dom_subcnt',\n",
      "       'dom_level', 'dom_length'],\n",
      "      dtype='object')\n",
      "reading features per from:  dummydata/features/2021-03-30/features_per.parquet\n",
      "Index(['host', 'mean_fqdn_period', 'max_fqdn_period', 'min_fqdn_period',\n",
      "       'std_fqdn_period', 'min_per', 'max_per', 'std_per', 'mean_per',\n",
      "       'cisco_min_period', 'cisco_max_period', 'cisco_mean_period',\n",
      "       'cisco_median_period', 'cisco_ratio_period', 'cisco_score',\n",
      "       'fqdn_popularity'],\n",
      "      dtype='object')\n",
      "reading features len2mal from:  dummydata/features/2021-03-30/features_len2mal.parquet\n",
      "Index(['host', 'minlen2malFQDN', 'avglen2malFQDN', 'maxlen2malFQDN'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "datadf = pd.DataFrame()\n",
    "\n",
    "for feat_typ in features_typs:\n",
    "    feat_fpath = os.path.join(feature_dir, features_fmt.format(feat_typ))\n",
    "    print(\"reading features {} from: \".format(feat_typ), feat_fpath)\n",
    "    \n",
    "    tmpfeat = pd.read_parquet(feat_fpath).fillna(0)\n",
    "    print(tmpfeat.columns)\n",
    "    \n",
    "    for col in tmpfeat.columns:\n",
    "        if col in drop_col:\n",
    "            tmpfeat = tmpfeat.drop(columns=col)\n",
    "            print(\"drop\", col)\n",
    "    \n",
    "    if len(datadf.columns) == 0:\n",
    "        datadf = tmpfeat\n",
    "    else:\n",
    "        datadf = datadf.merge(tmpfeat, on=\"host\", how=\"left\")\n",
    "        \n",
    "        \n",
    "datadf.to_parquet(\"dummydata/features/2021-03-30/features_{}.parquet\".format(logday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['host', 'cntMalFQDNs', 'cntIP', 'cntFQDN', 'sumMalEng', 'maxMalEng',\n",
       "       'avgMalEng', 'minMalEng', 'maxCisco', 'minCisco', 'avgCisco', 'sumMal',\n",
       "       'maxMal', 'minMal', 'avgMal', 'malFQDN_ratio', 'malEng_ratio',\n",
       "       'sum_ipDom', 'sum_ipMalDom', 'avg_ipDom', 'avg_ipMalDom', 'max_ipDom',\n",
       "       'max_ipMalDom', 'min_ipDom', 'min_ipMalDom', 'max_ipMalDomRatio',\n",
       "       'max_ipDomMalEngRatio', 'min_ipMalDomRatio', 'min_ipDomMalEngRatio',\n",
       "       'avg_ipMalDomRatio', 'avg_ipDomMalEngRatio', 'sum_ipDomMalEng',\n",
       "       'max_ipDomMalEng', 'min_ipDomMalEng', 'avg_ipDomMalEng',\n",
       "       'hist_malscore_min_period', 'hist_malscore_max_period',\n",
       "       'hist_malscore_mean_period', 'hist_malscore_median_period',\n",
       "       'hist_malscore_ratio_period', 'freq', 'occ', 'psd_ratio', 'dom_illegal',\n",
       "       'dom_sld_entropy', 'subdom_entropy', 'dom_entropy', 'fqdn_entropy',\n",
       "       'dom_tldcnt', 'dom_sldcnt', 'dom_subcnt', 'dom_level', 'dom_length',\n",
       "       'mean_fqdn_period', 'max_fqdn_period', 'min_fqdn_period',\n",
       "       'std_fqdn_period', 'min_per', 'max_per', 'std_per', 'mean_per',\n",
       "       'cisco_min_period', 'cisco_max_period', 'cisco_mean_period',\n",
       "       'cisco_median_period', 'cisco_ratio_period', 'cisco_score',\n",
       "       'fqdn_popularity', 'minlen2malFQDN', 'avglen2malFQDN',\n",
       "       'maxlen2malFQDN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
